{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-1.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give a model\n",
    "# give a reward function\n",
    "# step function\n",
    "# optimise function\n",
    "\n",
    "class GRPO_agent():\n",
    "\n",
    "    def __init__(self, model, tokenizer, chat_template: str, amount_of_answers: int = 5):\n",
    "        self.model = model\n",
    "        self.reference_model = None #model\n",
    "        self.chat_template = chat_template\n",
    "        self.tokenizer = tokenizer\n",
    "        self.amount = amount_of_answers\n",
    "\n",
    "    def get_action(self, prompt):\n",
    "        # Do I only sample from the value model or also from reference model?\n",
    "        # Maybe make it optionaL?\n",
    "        # More effictient way of getting value + better naming?\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.chat_template},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        model_inputs = tokenizer([text] * self.amount, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=512,\n",
    "            num_return_sequences=self.amount\n",
    "        )\n",
    "\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        answers = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        return answers\n",
    "\n",
    "    def optimise(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"TIGER-Lab/AceCode-87K\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])\n",
    "print(type(dataset))\n",
    "# https://huggingface.co/datasets/TIGER-Lab/AceCode-87K\n",
    "# example dataset https://www.oxen.ai/ox/Rust/file/main/results/GRPO_82_2025-03-02_22-49-17_Qwen2.5-Coder-1.5B-Instruct/results_code_and_tests.parquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(dataset,\n",
    "                         batch_size = 1,\n",
    "                         shuffle = True\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a pragmatic Rust programmer who enjoys test driven development. Given the following question, write a Rust function to complete the task. Make the code simple and easy to understand. The code should pass `cargo build` and `cargo clippy`. Try to limit library usage to the standard library std. Be careful with your types, and try to limit yourself to the basic built in types and standard library functions. When writing the function you can think through how to solve the problem and perform reasoning in the comments above the function.\n",
    "\n",
    "    Then write unit tests for the function you defined. Write multiple unit tests for the function. The tests should be a simple line delimited list of assert! or assert_eq! statements. When writing the unit tests you can have comments specifying what you are testing in plain english. The tests should use super::*.\n",
    "\n",
    "\n",
    "    An example output should look like the following:\n",
    "\n",
    "    ```rust\n",
    "    /// Reasoning goes here\n",
    "    /// and can be multi-line\n",
    "    fn add_nums(x: i32, y: i32) -> i32 {\n",
    "      x + y\n",
    "    }\n",
    "\n",
    "    #[cfg(test)]\n",
    "    mod tests {\n",
    "        use super::*;\n",
    "\n",
    "        #[test]\n",
    "        fn test_add_nums() {\n",
    "            // Test adding positive numbers\n",
    "            assert_eq!(add_nums(4, 2), 6);\n",
    "            // Test adding a positive and negative number\n",
    "            assert_eq!(add_nums(4, -2), 2);\n",
    "            // Test adding two negative numbers\n",
    "            assert_eq!(add_nums(-12, -1), -13);\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Make sure to only respond with a single  ```rust``` block. The unit tests must be defined inside the mod tests {} module. Make sure to import any standard library modules that you need. Do not add a main function.\n",
    "    \"\"\"\n",
    "\n",
    "x = GRPO_agent(model, tokenizer, SYSTEM_PROMPT, 2)\n",
    "\n",
    "for k, prompt_batch in enumerate(data_loader):\n",
    "    if k == 4:\n",
    "        break\n",
    "    q = prompt_batch[\"question\"][0]\n",
    "    q.replace(\"Python\", \"Rust\")\n",
    "\n",
    "\n",
    "    action = x.get_action(q)\n",
    "\n",
    "    # In common rl settings we send action to environment and use the output as input for our next run\n",
    "    # In this case we run output through the model and get the next action\n",
    "    # We use that to calculate the reward and update the model\n",
    "\n",
    "    # The \"environment\" tries to run the code and tests and gives a reward based on the output\n",
    "\n",
    "\n",
    "    reward, compiler_output = env(action)\n",
    "\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(q)\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    for i in action:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 522), match='```rust\\nfn sort_list(mut list: Vec<i32>) -> Vec<>\n",
      "True True True 0.6\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "rustcode = '''```rust\n",
    "fn sort_list(mut list: Vec<i32>) -> Vec<i32> {\n",
    "    list.sort();\n",
    "    list\n",
    "}\n",
    "\n",
    "#[cfg(test)]\n",
    "mod tests {\n",
    "    use super::*;\n",
    "\n",
    "    #[test]\n",
    "    fn test_sort_list() {\n",
    "        let unsorted = vec![5, 3, 8, 1, 2];\n",
    "        let sorted = sort_list(unsorted.clone());\n",
    "        assert_eq!(sorted, vec![1, 2, 3, 5, 8]);\n",
    "        assert_eq!(sorted, vec![1, 2, 3, 5, 8]);\n",
    "        assert_eq!(sorted, vec![1, 2, a3, 5, 8]);\n",
    "        assert_eq!(sorted, vec![1, 2, a3, 5, 8]);\n",
    "        assert_eq!(sorted, vec![1, 2123, a3, 5, 8]);\n",
    "    }\n",
    "}\n",
    "```'''\n",
    "\n",
    "def extract_rust_code(text: str) -> Optional[str]:\n",
    "    pattern = r'```rust\\n(.*?)\\n```'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    print(match)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "# How useful is this? To combat reward hacking?\n",
    "# Maybe just check if non empty\n",
    "def check_code_not_empty(code: str) -> bool:\n",
    "    if len(code) > 10:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_code_block(code: str) -> bool:\n",
    "    if extract_rust_code(code):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_test_block(code: str) -> bool:\n",
    "    pattern = r'(#\\[cfg\\(test\\)\\]\\s*mod\\s+tests\\s*\\{.*?\\})'\n",
    "    match = re.search(pattern, code, re.DOTALL)\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def response_contains_asserts(code: str) -> float:\n",
    "    pattern = r'#\\[cfg\\(test\\)\\]\\s*mod\\s+tests\\s*\\{([^}]*)\\}'\n",
    "    match = re.search(pattern, code, re.DOTALL)\n",
    "\n",
    "    if not match:\n",
    "        return 0.0\n",
    "    \n",
    "    test_block = match.group(0)\n",
    "\n",
    "    # Find all assert statements\n",
    "    assert_pattern = r'assert(?:_eq)?\\!(.*?);'\n",
    "    all_asserts = re.findall(assert_pattern, test_block)\n",
    "    total_asserts = len(all_asserts)\n",
    "    \n",
    "    if total_asserts == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    # Store unique assert statements\n",
    "    unique_asserts = set(assert_stmt.strip() for assert_stmt in all_asserts)\n",
    "    \n",
    "    return len(unique_asserts) / total_asserts\n",
    "\n",
    "# code running rewards and output rewards\n",
    "# This should all be in the environment\n",
    "def get_rewards(code: str):\n",
    "    total_reward = {\"not empty\": 0, \"code block\": 0, \"test block\": 0, \"asserts\": 0}\n",
    "    if check_code_not_empty(code):\n",
    "        total_reward[\"not empty\"] = 1\n",
    "    if check_code_block(code):\n",
    "        total_reward[\"code block\"] = 1\n",
    "    if check_test_block(code):\n",
    "        total_reward[\"test block\"] = 1\n",
    "    total_reward[\"asserts\"] = response_contains_asserts(code)\n",
    "    return total_reward\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
