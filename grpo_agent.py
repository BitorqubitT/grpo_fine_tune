import torch
import torch.optim as optim
from torch.nn.functional import kl_div, log_softmax
from utils import get_logprobs
import gc

class GRPO_agent():

    def __init__(self, model, tokenizer, chat_template: str, amount_of_answers: int = 5, memory=None, lr=1e-5):
        self.model = model
        self.reference_model = model.clone()
        self.reference_mode.eval()
        self.memory = memory
        self.chat_template = chat_template
        self.tokenizer = tokenizer
        self.amount = amount_of_answers
        self.device = "cuda"
        self.optimizer = optim.AdamW(self.model.parameters(), lr=lr)
        self.kl_clip = 0.1
        self.clip_eps = 0.2   #Used in deepseek
        self.kl_coef = 0.1 #Used in deepseek
        self.num_steps = 3

    def get_action(self, prompt) -> tuple:

        """
        answers:  human-readable text (useful for logging or reward computation)
        model_inputs.input_ids: prompt
        generated_ids: what the model did aka answer
        generated_full_ids: full sequence, useful for recovering the original generation context
        """

        messages = [
            {"role": "system", "content": self.chat_template},
            {"role": "user", "content": prompt}
        ]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        model_inputs = self.tokenizer([text] * self.amount, return_tensors="pt", padding=True).to(self.device)
        
        self.model.config.pad_token_id = self.tokenizer.pad_token_id

        attention_mask = (model_inputs.input_ids != self.tokenizer.pad_token_id).long()
        prompt_lengths = attention_mask.sum(dim=1).tolist()

        generated_full_ids = self.model.generate(
            model_inputs.input_ids,
            attention_mask=attention_mask,
            max_new_tokens=512,
            #do_sample=True,
            #top_k=50,
            #top_p=0.95,
            #temperature=1.0,
            num_return_sequences=self.amount
        )

        generated_ids = [
            output_ids[prompt_len:]
            for output_ids, prompt_len in zip(generated_full_ids, prompt_lengths)
        ]

        answers = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

        return answers, model_inputs.input_ids, generated_full_ids, generated_ids

    def optimise_network(self):
        # We dont want the raw prompt, we want the input_ids
        # And actions are the models answers
        input_ids, actions, old_logprobs, advantages = self.memory.get_values()
        total_loss = 0.0

        if isinstance(actions, list):
            actions = torch.stack(actions, dim=0)
        if isinstance(old_logprobs, list):
            old_logprobs = torch.cat(old_logprobs, dim=0).to(self.device) # [batch_size, seq_len]

        for _ in range(self.num_steps):
            # Should i use new or old model here?

            new_logprobs = get_logprobs(self.model, input_ids, actions, self.tokenizer, False)
            
            # We currently use total logprobs, so for the whole sequence.
            # Look at advanatage of doing it per token

            #Values are toooooo big 
            #Lets use logspace operatoins:
            #ratio = torch.exp(new_logprobs - old_logprobs)
            ratio_log = new_logprobs - old_logprobs
            ratio = torch.exp(torch.clamp(ratio_log, -10, 10))

            print("new_logprobs", new_logprobs)
            print("old_logprobs", old_logprobs)
            print("ratio", ratio)
            # PPO-style clipped loss
            clipped_ratio = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps)
            loss_unclipped = ratio * advantages
            loss_clipped = clipped_ratio * advantages
            policy_loss = -torch.mean(torch.min(loss_unclipped, loss_clipped))

            #TODO: Put in ref mode
            ref_logprobs = get_logprobs(self.reference_model, input_ids, actions, self.tokenizer, False)
            

            kl_div = ref_logprobs - new_logprobs
            kl_loss = torch.mean(torch.clamp(kl_div, max=self.kl_clip))
            total_loss = policy_loss + self.kl_coef * kl_loss
            print("policy_loss:", policy_loss.item())
            print("kl_loss:", kl_loss.item())
            print("total_loss:", total_loss.item())

            self.optimizer.zero_grad()
            total_loss.backward()
            self.optimizer.step() 

            #TODO: chekc these
            del new_logprobs, ref_logprobs, ratio, clipped_ratio, policy_loss, kl_loss, total_loss
            gc.collect()
            torch.cuda.empty_cache()


"""
Each sample is one response generated by the policy for a prompt.
prompt_input_ids: tokenized input IDs of the prompt.
response_input_ids: tokenized output IDs of the response.
log_prob: log probability of the response under the current policy (used in GRPO loss)
reward: scalar reward assigned to this response.
group_id: an identifier to group responses by prompt (3 responses per prompt = 1 group).        
"""

#TODO: Optimise this code, now i just overwrite the memory
class Memory():
    """ 
        Class that holds memory for ppoagent
    """
    def __init__(self, num_steps: int, num_envs: int, device: torch.device, logits_shape):
        #self.obs = torch.zeros((num_steps, num_envs) + (9,)).to(device)
        self.device = device
        self.num_envs = num_envs
        self.num_steps = num_steps
        self.input_ids = torch.zeros((num_steps, num_envs) + (9,)).to(device)
        self.actions = torch.zeros((num_steps, num_envs) + (9,)).to(device)
        self.logprobs = []
        self.advantages = torch.zeros((num_steps, num_envs) + (9,)).to(device)

    def update_values(self, input_ids, actions, logprobs, advantages):
        self.input_ids = input_ids
        self.actions = actions
        self.logprobs.append(logprobs)
        self.advantages = advantages
    
    def get_values(self) -> tuple:
        return self.input_ids, self.actions, self.logprobs, self.advantages
    
    def clear(self):
        self.input_ids = torch.zeros((self.num_steps, self.num_envs) + (9,)).to(self.device)
        self.actions = torch.zeros((self.num_steps, self.num_envs) + (9,)).to(self.device)
        self.logprobs = []
        self.advantages = torch.zeros((self.num_steps, self.num_envs) + (9,)).to(self.device)
