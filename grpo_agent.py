import torch
import torch.optim as optim
from torch.nn.functional import kl_div, log_softmax

class GRPO_agent():

    def __init__(self, model, tokenizer, chat_template: str, amount_of_answers: int = 5, memory=None, lr=1e-5):
        self.model = model
        #self.reference_model = model.clone()
        #self.reference_mode.eval()
        self.memory = memory
        self.chat_template = chat_template
        self.tokenizer = tokenizer
        self.amount = amount_of_answers
        self.device = "cuda"
        self.optimizer = optim.AdamW(self.model.parameters(), lr=lr)
        self.kl_clip = 0.1
        self.clip_eps = 0.2   #Used in deepseek
        self.kl_coef = 0.1 #Used in deepseek
        self.num_steps = 5

    def get_action(self, prompt):
        # More effictient way of getting value + better naming?
        messages = [
            {"role": "system", "content": self.chat_template},
            {"role": "user", "content": prompt}
        ]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        model_inputs = self.tokenizer([text] * self.amount, return_tensors="pt", padding=True).to(self.device)
        prompt_lengths = [input_ids.shape[0] for input_ids in model_inputs.input_ids]
        
        generated_full_ids = self.model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,
            num_return_sequences= self.amount
        )

        generated_ids = [
            output_ids[prompt_len:]
            for output_ids, prompt_len in zip(generated_full_ids, prompt_lengths)
        ]

        answers = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

        """
        answers:  human-readable text (useful for logging or reward computation)
        model_inputs.input_ids: prompt
        generated_ids: what the model did
        generated_full_ids: full sequence, useful for recovering the original generation context
        """
        return answers, model_inputs.input_ids, generated_full_ids, generated_ids
    
    # TODO: CHECK THIS
    def optimise_network(self):
        # We dont want the raw prompt, we want the input_ids
        # And actions are the models answers
        input_ids, actions, old_logprobs, advantages = self.memory.get_values()
        total_loss = 0.0

        # TODO: we have multiple actions we flatten the batch?
        # TODO: Think about data types and shapes. Make them as consistent as possible
        if isinstance(actions, list):
            actions = torch.stack(actions, dim=0)
        if isinstance(old_logprobs, list):
            old_logprobs = torch.cat(old_logprobs, dim=0).to(self.device) # [batch_size, seq_len]

        for _ in range(self.num_steps):
            print("steppppppppppp")
            outputs = self.model(input_ids, return_dict=True)
            logits = outputs.logits

            # We currently use total logprobs, so for the whole sequence.
            # Look at advanatage of doing it per token
            logprobs = torch.nn.functional.log_softmax(logits, dim=-1).to(self.device)
            action_logprobs = torch.gather(logprobs, 2, actions.unsqueeze(-1)).squeeze(-1)  # [B, T]

            total_action_logprobs = action_logprobs.sum(dim=-1)  # [B]

            # Compute importance sampling ratio
            ratio = torch.exp(total_action_logprobs - old_logprobs)

            # PPO-style clipped loss
            clipped_ratio = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps)
            loss_unclipped = ratio * advantages
            loss_clipped = clipped_ratio * advantages
            policy_loss = -torch.mean(torch.min(loss_unclipped, loss_clipped))

            #with torch.no_grad():
            #    ref_outputs = self.ref_model(input_ids=input_ids)
            #    ref_logits = ref_outputs.logits
            #    ref_logprobs = torch.nn.functional.log_softmax(ref_logits, dim=-1)

            #kl = torch.nn.functional.kl_div(
            #    input=logprobs,
            #    target=ref_logprobs,
            #    reduction="batchmean",
            #    log_target=True,
            #)
            #kl_loss = torch.clamp(kl, max=self.kl_clip)
            # Combine losses if using KL penalty
            total_loss += policy_loss + self.kl_coef * 1

        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()


"""
Each sample is one response generated by the policy for a prompt.
prompt_input_ids: tokenized input IDs of the prompt.
response_input_ids: tokenized output IDs of the response.
log_prob: log probability of the response under the current policy (used in GRPO loss)
reward: scalar reward assigned to this response.
group_id: an identifier to group responses by prompt (3 responses per prompt = 1 group).        
"""

class Memory():
    """ 
        Class that holds memory for ppoagent
    """
    def __init__(self, num_steps: int, num_envs: int, device: torch.device, logits_shape):
        #self.obs = torch.zeros((num_steps, num_envs) + (9,)).to(device)
        self.device = device
        self.num_envs = num_envs
        self.num_steps = num_steps
        self.input_ids = torch.zeros((num_steps, num_envs) + (9,)).to(device)
        self.actions = torch.zeros((num_steps, num_envs) + (9,)).to(device)
        self.logprobs = []
        self.advantages = torch.zeros((num_steps, num_envs) + (9,)).to(device)

    def update_values(self, input_ids, actions, logprobs, advantages):
        self.input_ids = input_ids
        self.actions = actions
        self.logprobs.append(logprobs)
        self.advantages = advantages
    
    def get_values(self):
        return self.input_ids, self.actions, self.logprobs, self.advantages
    
    def clear(self):
        self.input_ids = torch.zeros((self.num_steps, self.num_envs) + (9,)).to(self.device)
        self.actions = torch.zeros((self.num_steps, self.num_envs) + (9,)).to(self.device)
        self.logprobs = []
        self.advantages = torch.zeros((self.num_steps, self.num_envs) + (9,)).to(self.device)
